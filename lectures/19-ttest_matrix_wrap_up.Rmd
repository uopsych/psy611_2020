---
title: 'Comparing two means and fall term wrap-up'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts, "my-theme.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
library(papaja)
library(ggpubr)
library(fivethirtyeight)
library(knitr)
library(kableExtra)
```
## Annoucements

* Thank you, Max!
* The GEs and I are available by appointment through finals week. 

* Reminder: Homework 3 due next Friday (9am)
  * Grading starts this weekend: expect corrections within 24 hours.
  
---
## Last time...

* Effect sizes and assumptions of _t_-tests

  * Cohen's D
  * Overlap of population distributions
  
  * Independence
  * Homogeneity of variance
  * Normality
  
---

## Example

In the country of Panem, there are 13 (or 14?) distinct geographic districts, each of which specializes in a particular trade or industry. The Capital is the most technologically advanced and is home to wealthy elites (CEOs, politicians, university professors, etc). District 12 is a coal-mining district. 

Researchers asked whether these two geographic regions differ in their exposure to daily life stress. Citizens of these districts were randomly sampled and completed the [Survey of Recent Life Experiences](https://link.springer.com/article/10.1007/BF00848327), which measures daily hassles. Scores can range from 0 (no daily hassles) to 160 (constant daily hassles).

What are the null and alternative hypotheses? What kind of test will we run?

---

| Capital | District 12 |
|:-------:|:-----------:|
| 12      | 100         |
| 72      | 92         |
| 81      | 110         |
| 69      | 99         |
| 44      | 80         |
| 61      | 96         |
| 55      | 102         |
| 43      | 98         |
| 60      | 71         |
| 52      | 86         |

---
```{r}
panem = data.frame(
  district = rep(c("Capital", "12"), each = 10),
  srle = c(
    12,72,81,69,44,61,55,43,60,52,
    100,92,110,99,80,96,102,98,71,86)
)

psych::describeBy(panem, group = panem$district)
```

```{r, echo = F, messsrle = F, warning = F, results ='hide'}
sum_stats = panem %>%
  group_by(district) %>%
  summarize(
    n = n(),
    m = mean(srle),
    sd = sd(srle)
  )
```

---
.pull-left[
**Capital**  
$N = `r printnum(sum_stats$n[1])`$  
$M = `r printnum(sum_stats$m[1])`$  
$SD = `r printnum(sum_stats$sd[1])`$]

.pull-right[
**District 12**  
$N = `r printnum(sum_stats$n[2])`$  
$M = `r printnum(sum_stats$m[2])`$  
$SD = `r printnum(sum_stats$sd[2])`$]

* Set an alpha-level. 

* Define sampling distribution. 

$$\mu_D = \mu_0$$
$$\sigma_D = ??$$
--

The standard error of the difference in means is the $\frac{\text{population standard deviation}}{\sqrt{N}}$.

But we have two sample sizes, and we have two estimates of the population SD.

---

```{r, echo = F}
pool_var = ((9*sum_stats$sd[1]^2) + (9*sum_stats$sd[2]^2))/(18)
pool_sd = sqrt(pool_var)
```


Calculate pooled variance (to get our best estimate of the population variance):

$$\large{\hat{\sigma}^2_p = \frac{(N_1-1)\hat{\sigma}^2_1 + (N_2-1)\hat{\sigma}^2_2}{N_1+N_2-2}}$$
$$\large{\hat{\sigma}^2_p = \frac{(10-1)`r printnum(sum_stats$sd[1])`^2 + (10-1)`r printnum(sum_stats$sd[2])`^2}{10+10-2}} = `r printnum(pool_var)`$$
Don't forget to take the square root to get our SD estimate.

$$\large{\hat{\sigma}_p = \sqrt{\hat{\sigma}^2_p} = \sqrt{`r printnum(pool_var)`} = `r printnum(sqrt(pool_var))`} $$

---

```{r, echo = F}
se = sqrt(pool_var)*sqrt((1/10) + (1/10))
```


Deal with two sample sizes by weighting by $\sqrt{\frac{1}{N_1} + \frac{1}{N_2}}$ instead of $\frac{1}{\sqrt{N}}$.

$$\sigma_D = \sigma_p\sqrt{\frac{1}{N_1} + \frac{1}{N_2}} = `r printnum(sqrt(pool_var))`\sqrt{\frac{1}{10} + \frac{1}{10}} = `r printnum(se)`$$

```{r, echo = F}
t = (sum_stats$m[1] - sum_stats$m[2])/se
```

Calculate our _t_-statistic:

$$t = \frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sigma_D} = \frac{`r printnum(sum_stats$m[1])` - `r printnum(sum_stats$m[2])` - 0}{`r printnum(se)`} = `r printnum(t)`$$
---

```{r}
t.test(srle~district, data = panem, var.equal = T)
```

---

```{r, echo = F}
cohend = (sum_stats$m[1] - sum_stats$m[2])/sqrt(pool_var)
```

Is this a large difference? 

$d = \frac{\bar{X}_1 - \bar{X}_2}{\sigma_p} = \frac{`r printnum(sum_stats$m[1])` - `r printnum(sum_stats$m[2])` - 0}{`r printnum(sqrt(pool_var))`} = `r printnum(cohend)`$

```{r}
(cohensd = lsr::cohensD(srle~district, data = panem, method = "pooled"))
```

---

How much power did we have to detect this effect?

```{r, echo = F}
mu = 0
s = sqrt(pool_var)
sem = se

mean = sum_stats$m[1] - sum_stats$m[2]

cv = qnorm(mean = mu, sd = se, p = .025, lower.tail = F)


ggplot(data.frame(x = seq(-20, 60)), aes(x)) +
  stat_function(fun = function(x) dnorm(x, m = mu, sd = sem)) +
  
  stat_function(fun = function(x) dnorm(x, m = mean, sd = sem),
                geom = "area", xlim = c(cv, 60), 
                aes(fill = "Power"), alpha = .5) +
  stat_function(fun = function(x) dnorm(x, m = mean, sd = sem),
                geom = "area", xlim = c(-20, cv), 
                aes(fill = "Type II Error"), alpha = .5) +
  stat_function(fun = function(x) dnorm(x, m = mu, sd = sem),
                geom = "area", xlim = c(cv, 60), 
                aes(fill = "Type I Error"), alpha = .5) +
  stat_function(fun = function(x) dnorm(x, m = mean, sd = sem)) +
  geom_vline(aes(xintercept = mu))+
  geom_vline(aes(xintercept = mean))+
  geom_hline(aes(yintercept = 0))+
  scale_x_continuous("Difference in means", breaks = seq(-20,60,10)) +
  scale_y_continuous(NULL, breaks = NULL) + 
  theme_pubr() +
  theme( text = element_text(size = 20),
    legend.position = "bottom")
```

---

We can calculate this by hand: remember, this is the area under our alternative distribution that is from the critical value and farther away from our mean.

```{r}
(critical_value = qnorm(mean = 0, 
                        sd = 7.098, 
                        p = .025, 
                        lower.tail = F))
pnorm(q = critical_value, 
      mean = 38.5, 
      sd = 7.098, 
      lower.tail = F)
```

---

But you don't need to do things by hand:

```{r}
library(pwr)
pwr.t.test(n = 10, 
           d = cohensd, 
           sig.level = .05, 
           type = "two")
```

The arguments for the `pwr.t.test` function are sample size (`n`), effect size (`d`), alpha level (`sig.level`), and power (`power`). Give it any three, and it will return the fourth!

---



This is very useful. For example, if we wanted to replicate this study, how many participants would we need? We can use the same function to find out:

```{r}
pwr.t.test(
  d = cohensd,
  sig.level = .05, 
  power = .90, 
  type = "two"
)
```

---

Did we meet the assumptions of this test?

--

```{r, warning = F}
car::leveneTest(srle~district, data = panem, center = "mean")
```

--

```{r, echo = 1, fig.height = 2, fig.width=8}
shapiro.test(x = panem$srle)
panem %>% ggplot(aes(x = srle)) + geom_histogram(bins = 5, fill = "purple", color = "white") + theme_pubr()
```

---
## Remember matrix algebra?

In week 3, we covered the mechanics of matrix algebra, including addition/subtraction and multiplication. 

The important takeaway was that we can use tranformation matrices to create linear combinations of rows and columns. These linear combinations help us to summarize datasets in valuable ways.

$$y = 5x_1 + \frac{1}{7}x_2 - 12x_3$$

$$\large \mathbf{M} = \left[\begin{array}
{r}
5  \\
\frac{1}{7}  \\
-12  \\
\end{array}\right]$$

---

Transformation matrices applied to a data matrix using *pre*-multiplication creates linear combinations of rows -- this allows us to aggregate observations into meaningful "groups", like control/treatment, freshman/senior, time 1/time 2.

Transformation matrices applied to a data matrix using *post*-multiplication creates linear combinations of columns -- this allows us to aggregate variables into meaningful indices, like summing variables to get the number correct on a test or finding the average response to a set of items in a scale.

Using both pre- and post-multiplication _together_ is an efficient way of asking questions, like:
* are average levels of the CESD lower after an intervention compared to before?
* what is the average score on each of 4 quizzes?

---

## Example

```{r, echo = F}
set.seed(1203)
group = sample(c("A", "B"), size = 8, replace = T)
level = ifelse(group == "A", 3, 6)
fake_data = data.frame(
  ID = c(1:8),
  group = group,
  item1 = round(level + rnorm(n = 8, mean = 2)),
  item2 = round(level + rnorm(n = 8, mean = -1)),
  item3 = round(level + rnorm(n = 8, mean = 2)),
  item4 = round(level + rnorm(n = 8, mean = -2))
)
```

```{r}
fake_data
```

```{r}
T_pre = matrix(
  c( 1,0,0,1,1,0,1,0,
     0,1,1,0,0,1,0,1), 
  byrow = T, nrow = 2
)
```

---

```{r}
T_pre %*% as.matrix(fake_data[,-c(1:2)])
.25*T_pre %*% as.matrix(fake_data[,-c(1:2)])
```

---

```{r}
T_post = matrix(
  c(1/4,
    1/4,
    1/4,
    1/4),
  ncol = 1
)

as.matrix(fake_data[,-c(1:2)]) %*% T_post
```

---

```{r}
.25*T_pre %*% as.matrix(fake_data[,-c(1:2)]) %*% T_post
```

---

## *t*-tests and Matrix Algebra

### Independent-samples *t*-test

At it's heart, the independent samples *t*-test proposes a model in which our best guess of a person's score is the mean of their district. 

We can use **linear combinations** of both rows and columns to generate these "best guess" scores. 

---
Assume your data take the following form:

.pull-left[

$$\large \mathbf{X} = \left(\begin{array}
{rr}
\text{district} & \text{SRLE} \\
Cap & 12  \\
D12 & 100  \\
\vdots & \vdots  \\
Cap & 72 
\end{array}\right)$$
]

.pull-right[
What are your linear combinations of columns?


What are your linear combinations of rows?
]

--

**Linear combinations of columns:** 

$$\large \mathbf{T_C} = \left(\begin{array}
{r}
0 \\
1
\end{array}\right)$$

---
Assume your data take the following form:

.pull-left[

$$\large \mathbf{X} = \left(\begin{array}
{rr}
\text{district} & \text{SRLE} \\
Cap & 12  \\
D12 & 100  \\
\vdots & \vdots  \\
Cap & 72 
\end{array}\right)$$
]

.pull-right[
What are your linear combinations of columns?


What are your linear combinations of rows?
]



**Linear combinations of rows:** 

$$\mathbf{T_R} = \left(\begin{array}
{rrrr}
\frac{1}{N_C} & 0 & \dots & \frac{1}{N_C} \\
0 & \frac{1}{N_{D12}} & \dots & 0 \\
\end{array}\right)$$

---

$$(T_R)X(T_C) = M$$

This set of linear combinations calculates the different means of our two districts. That's great! 


But there's another way. We can create a single transformation matrix, $\large \mathbf{T}$, that's very simple:

$$\mathbf{T} = \left(\begin{array}
{rr}
1 & 0 \\
1 & 1 \\
\vdots & \vdots  \\
1 & 0 \\
\end{array}\right)$$

Using this matrix, we can solve a different equation:

$$(T'T)^{-1}T'X = (b)$$
In this case, *b* will be a vector with 2 scalars.

---


```{r, echo = 1:3}
X = panem$srle
T_mat = matrix(1, ncol = 2, nrow = length(X))
T_mat[panem$district == "Capital", 2] = 0
T_mat
```

---

$$(T'T)^{-1}T'X = (b)$$

```{r}
solve(t(T_mat) %*% T_mat) %*% t(T_mat) %*% X
```

What do these numbers represent?

???

$$\Large \bar{X} = T\mathbf{X} = 1'\mathbf{X}(1'1)^{-1}$$

---

### Standard errors

We can use the same matrix, $\large \mathbf{T}$ , to transform the variance-covariance into standard errors.

$$\large  \text{Var}_b = \hat{\sigma}^2(T'T)^{-1}$$

We use the pooled variance, instead of the empirical estimate. 
```{r}
var1 = var(panem$srle[panem$district == "Capital"])
var2 = var(panem$srle[panem$district == "12"])
n1 = length(which(panem$district == "Capital"))
n2 = length(which(panem$district == "12"))
pooled = ((n1-1)*var1 +(n2-1)*var2)/(n1+n2-2)
```

```{r}
pooled * solve(t(T_mat) %*% T_mat)
```

---

.pull-left[
```{r, echo = 1}
solve(t(T_mat) %*% T_mat) %*% t(T_mat) %*% X
b = solve(t(T_mat) %*% T_mat) %*% t(T_mat) %*% X
```
]

.pull-right[
```{r, echo = 1}
pooled * solve(t(T_mat) %*% T_mat)
var_b = pooled * solve(t(T_mat) %*% T_mat)
```
]

--

.pull-left[
$$t_{b_1} = \frac{54.9}{\sqrt{25.196}} = `r round(b[1,1]/sqrt(var_b[1,1]),3)`$$



This is a one-sample *t*-test comparing the mean of the reference group to 0.

]

--

.pull-right[
$$t_{b_2} = \frac{38.5}{\sqrt{50.392}} = `r round(b[2,1]/sqrt(var_b[2,2]),3)`$$


This is an independent-samples *t*-test comparing the mean of the two groups
]

---

## *t*-tests and Matrix Algebra

### Paired-samples *t*-test

The algebra for the paired-samples *t*-test is actually quite simple, because the difference score is a very simple weighted linear combination:

$$LC_i = W_1X_{1,i} + W_2X_{2,i} + \dots + W_kX_{k,i}$$

The variance of a weighted linear combination is a function of the weights (W) applied the variance-covariance matrix:

.pull-left[

$$\large \left[\begin{array}
{rr} 
W_1 & W_2
\end{array}\right]
\left[\begin{array}
{rr} 
\hat{\sigma}^2_{X_1} & \hat{\sigma}_{X_1X_2} \\
\hat{\sigma}_{X_2X_1} & \hat{\sigma}^2_{X_1}
\end{array}\right]
\left[\begin{array}
{r} 
W_1 \\
W_2
\end{array} 
\right]$$

]

???
What is the linear combination?

Difference = At - Away

Difference = (1)At + (-1)Away

---

```{r, echo = F}
gulls = read.delim(here("data/gulls/pairs.txt"))
```

```{r}
X = as.matrix(gulls[, c("At", "Away")])
W = c(1, -1)
```

.pull-left[
Difference scores:

```{r}
X %*% W
```
]

.pull-right[
Standard deviation of difference scores:

```{r}
(V_X = cov(X))
(V_Difference = W %*% V_X %*% W)
sqrt(V_Difference)
```

]

---

### Matrix Algebra

There is nothing particularly special about difference scores $(W = 1, -1)$. Any conceptually sensible weights can be used to create  linear combinations, which can then be used in hypothesis tests:

$$t = \frac{\bar{LC}-\mu}{\frac{\hat{\sigma}_{LC}}{\sqrt{N}}}$$

In other words: you can test **any linear combination** of variables against **any null hypothesis** using this formula. You are not limited to differences in means or nil hypotheses $\large (\mu=0)$.

---

Building and testing linear combinations of variables is nearly everything we'll do in PSY 612. 

Most statistical methods propose some model, represented by linear combinations, and then tests how likely we would be to seee data like ours under that model. 

* one-sample *t*-tests propose that a single value represents all the data

* *t*-tests propose a model that two means differ 

* more complicated models propose that some comination of variables, categorical and continuous, govern the patterns in the data

---

We'll spend most of PSY 612 talking about different ways of building and testing models, but at their heart, all will use a combination of 


$$(T'T)^{-1}T'X = (b)$$

$$LC_i = W_1X_{1,i} + W_2X_{2,i} + \dots + W_kX_{k,i}$$

--

As we think about where we're going next term, it will be helpful to put some perspective on what we've covered already.

---

background-image: url("images/wrap-up1.jpeg")
background-size: contain

---

background-image: url("images/wrap-up2.jpeg")
background-size: contain

---

background-image: url("images/wrap-up3.jpeg")
background-size: contain

---

background-image: url("images/wrap-up4.jpeg")
background-size: contain

---

background-image: url("images/wrap-up5.jpeg")
background-size: contain

---

background-image: url("images/wrap-up6.jpeg")
background-size: contain

---

Because we're using probability theory, we have to assume independence between trials.

* independent rolls of the die
* independent draws from a deck of cards
* independent... tests of hypotheses?


What happens when our tests are not independent of one another?

* For example, what happens if I only add a covariate to a model if the original covariate-less model is not significant?

* Similarly, what happens if I run multiple tests on one dataset?

---

The issue of independence between tests has long been ignored by scientists. It was only when we were confronted with impossible-to believe results (people have ESP, listening to "When I'm Sixty-Four" by The Beatles makes you younger) that we started to doubt our methods.

And an explosion of fraud cases made us revisit our incentive structures. Maybe we weren't producing the best science we could...

.pull-left[
![](images/replicability2.png)
]

.pull-right[

Since then (2011), there's been greater push to revisit "known" effects in psychology and related fields, with sobering results. 

The good news is that things seem to be changing, rapidly.
]

---

Independence isn't the only assumption we have to make to conduct hypothesis tests. 

We also make assumptions about the underlying populations.

![](images/null.png)
.pull-left[Normality]
.pull-right[Homogeneity of variance]

&nbsp;&nbsp;
Sometimes we fail to meet those assumptions, in which case we may have to change course. 


.small[Picture credit: Cameron Kay]

---

How big of a violation of the assumption is too much?

--

* It depends. 


How do you know your operation X measures your construct A well?

--

* You don't. 

What should alpha be?

--

* You tell me. 



---

.pull-left[
Statistics is not a recipe book. It's a tool box. And there are many ways to use your tools.

Your job is to 
1. Understand the tools well enough to know how and when to use them, and
2. Be able to justify your methods.

]

.pull-right[
![](images/tools.jpg)
]

It's worth adding to that, as a graduate student, you're long past the days of trying to memorize and regurgitate information in a class. You're training to be an independent scholar. For that reason, we've focused on helping you figure things out for yourself, which will be a far more useful skill than learning less material more deeply now. 

Teach a scientist to fish, or something like that.
---

class: inverse

## Next time...

PSY 612 in 2021

(But first):
* Quiz 9 now 
* Journal 10 due Monday night





